{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010db2f5-cde0-48bf-a902-5f88e4234072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pravin/Desktop/T5/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "print(\"Importing Libraries\")\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from transformers import (T5ForConditionalGeneration, \n",
    "AdamW, T5Tokenizer, TrainingArguments, Trainer,\n",
    "get_linear_schedule_with_warmup)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(\"Libraries Imported Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793f1b44-439a-4ae8-9c2b-2e461c9f3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/home/pravin/Desktop/April/7thapril/Sentiment/train_data_sentiment.csv\")\n",
    "df_train.dropna(inplace = True)\n",
    "df_train = df_train.head(50000)\n",
    "df_train = df_train.astype(str)\n",
    "df_train['context'] = df_train['context'].apply(lambda x: ' context: ' + x )\n",
    "df_train['question'] = df_train['question'].apply(lambda x: 'question: ' + x)\n",
    "df_train = df_train.to_dict(orient=\"list\")\n",
    "train_dataset = Dataset.from_dict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6acc25bc-6da5-4b81-b252-548386b1cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=4)\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2589c703-d865-4155-a783-18ea4da96ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9fd69a-d45b-484c-96f3-b81901d0e02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = 't5-large'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8e4ee2-d89c-4f60-95c7-44321d97c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(\n",
    "#     inference_mode=False,\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     target_modules=[\"q\", \"k\", \"v\"],\n",
    "#     init_lora_weights=\"gaussian\"\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3911393-ab4a-4a72-a756-5ba34f0598bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195e13c9-e3be-46b5-9a2b-af9e8f709de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b330ff4-e551-478f-8799-88377ccb0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,077,888 || all params: 744,745,984 || trainable%: 0.9503761217999398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 16)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 16)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906c0479-5c5d-461b-b835-566f9a7930b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ff5559-c86c-4a70-a48d-76da06646ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pravin/Desktop/T5/myenv/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(f\"runs/runs1/T5_large_lora_batch_{BATCH_SIZE}_lr_{LEARNING_RATE}\")\n",
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "data_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(train_dataset) * NUM_EPOCHS)\n",
    "model, optimizer, training_dataloader, scheduler = accelerator.prepare(model, optimizer, data_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a310a8f0-4751-460a-8934-367d1cef6139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "1it [00:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 0, epoch: 0, Loss = 0.00868138112127781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [15:39,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 1000, epoch: 0, Loss = 0.033623673021793365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [31:17,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 2000, epoch: 0, Loss = 0.010446126572787762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [46:57,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 3000, epoch: 0, Loss = 0.08629095554351807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:02:36,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 4000, epoch: 0, Loss = 0.06064416468143463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:18:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 5000, epoch: 0, Loss = 0.003860006108880043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:33:57,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch0, Batch number: 6000, epoch: 0, Loss = 0.08750130981206894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6250it [1:37:51,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 0, epoch: 1, Loss = 0.060578200966119766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [15:42,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 1000, epoch: 1, Loss = 0.0806998759508133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [31:21,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 2000, epoch: 1, Loss = 0.020019032061100006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [46:58,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 3000, epoch: 1, Loss = 0.2183934450149536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:02:36,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 4000, epoch: 1, Loss = 0.06624727696180344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:18:16,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 5000, epoch: 1, Loss = 0.014107747003436089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:33:54,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch1, Batch number: 6000, epoch: 1, Loss = 0.01813841424882412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6250it [1:37:47,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 0, epoch: 2, Loss = 0.28804463148117065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [15:39,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 1000, epoch: 2, Loss = 0.02263427898287773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [31:18,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 2000, epoch: 2, Loss = 0.14595651626586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [46:55,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 3000, epoch: 2, Loss = 0.08463691920042038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:02:31,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 4000, epoch: 2, Loss = 0.00425028195604682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:18:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 5000, epoch: 2, Loss = 0.05236300826072693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:33:43,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved in folder/home/pravin/Desktop/April/7thapril/Sentiment/second_iteration/epoch2, Batch number: 6000, epoch: 2, Loss = 0.011198504827916622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6250it [1:37:36,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_number, batch in tqdm(enumerate(training_dataloader)):\n",
    "        total_loss = 0.0\n",
    "        losses = []\n",
    "        avg_losses = []\n",
    "        with accelerator.accumulate(model):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = tokenizer( batch[\"question\"], batch[\"context\"], max_length = 512, padding='max_length', truncation='longest_first', return_tensors=\"pt\").input_ids.to(device)\n",
    "            attention_mask = tokenizer(batch[\"question\"], batch[\"context\"], max_length = 512, padding='max_length', truncation='longest_first', return_tensors=\"pt\").attention_mask.to(device)\n",
    "            labels = tokenizer(batch[\"answer\"], padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            avg_train_loss = (BATCH_SIZE * total_loss)/(batch_number+1)\n",
    "            losses.append(loss)\n",
    "            avg_losses.append(avg_train_loss)\n",
    "            for name, param in model.named_parameters():\n",
    "                if name == \"base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight\":\n",
    "                    writer.add_histogram(name, param,  global_step = batch_number)\n",
    "            writer.add_scalar('Training loss', loss, global_step = batch_number)\n",
    "            writer.add_scalar('avg_train_loss', avg_train_loss, global_step = batch_number)\n",
    "            if batch_number%1000 == 0:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"epoch{epoch}\")\n",
    "                model.save_pretrained(checkpoint_path)\n",
    "                print(f\"Model Saved in folder{checkpoint_path}, Batch number: {batch_number}, epoch: {epoch}, Loss = {loss}\")    \n",
    "    print(f\"Epoch {epoch + 1 } Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863f729-a3de-4318-9d95-ad8940f3974b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e0df62-4dcd-43c5-9440-371ae878e11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,077,888 || all params: 744,745,984 || trainable%: 0.9503761217999398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q\", \"k\", \"v\"],\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "model_1 = get_peft_model(model_1, peft_config)\n",
    "model_1.print_trainable_parameters()\n",
    "\n",
    "checkpoint = \"/home/pravin/Desktop/April/7thapril/checkpoint_large_LoRA_16qkv0.pt\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model_1.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c39dffa-7460-4de4-8384-260156a8c334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tillya'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\" question: What is his nick name? context: Hi, guys my name is Pravin but I go by Tillya and I live in Banglore \", return_tensors='pt').input_ids\n",
    "model_1.generate(input_ids = input_ids)\n",
    "tokenizer.decode(model_1.generate(inputs = input_ids)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac22fe1-d044-488d-ad21-ef5a59b81098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
